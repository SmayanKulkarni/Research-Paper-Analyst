{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bb9e50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers stored: 10\n",
      "                                               title  \\\n",
      "0  One Model, Many Languages: Meta-learning for M...   \n",
      "1  Latent linguistic embedding for cross-lingual ...   \n",
      "2  Speech Synthesis as Augmentation for Low-Resou...   \n",
      "3  Listening while Speaking and Visualizing: Impr...   \n",
      "4  Data Augmentation for Training Dialog Models R...   \n",
      "5  Improving Cross-Lingual Transfer Learning for ...   \n",
      "6  DNN-Based Semantic Model for Rescoring N-best ...   \n",
      "7  Mitigating the Impact of Speech Recognition Er...   \n",
      "8  Exploration of End-to-End ASR for OpenSTT -- R...   \n",
      "9  On the Comparison of Popular End-to-End Models...   \n",
      "\n",
      "                                 url                              paper_id  \n",
      "0  http://arxiv.org/abs/2008.00768v1  f5dfaf5b-b5cb-4180-9012-38a67d76065b  \n",
      "1  http://arxiv.org/abs/2010.03717v1  77695258-3d71-4d21-b2cc-87cd3d168bd1  \n",
      "2  http://arxiv.org/abs/2012.13004v1  d6dba606-822b-4e6d-80ae-cd75ce369d0a  \n",
      "3  http://arxiv.org/abs/1906.00579v3  2e901a47-3432-4647-a6c0-6a0bc66f3688  \n",
      "4  http://arxiv.org/abs/2006.05635v1  0e0d1070-3987-4e40-a9ac-ab06f0159ae9  \n",
      "5  http://arxiv.org/abs/2006.05474v2  eae5ae37-25ce-4eaa-afb5-5c5208b3a687  \n",
      "6  http://arxiv.org/abs/2011.00975v1  b7b953f2-c101-4548-8b30-eac78e90f89f  \n",
      "7  http://arxiv.org/abs/1904.07904v1  b5753acc-09d9-475a-b18c-c4f9e9c97c1a  \n",
      "8  http://arxiv.org/abs/2006.08274v2  14da6b25-9c84-4a2f-afd1-5200c60e1290  \n",
      "9  http://arxiv.org/abs/2005.14327v2  1992f64b-18a1-40dc-a899-119ca7f644d8  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "df = pd.read_parquet(\"/home/smayan/Desktop/Research-Paper-Analyst/backend/storage/parquet/papers.parquet\")\n",
    "\n",
    "# Show the data\n",
    "print(f\"Total papers stored: {len(df)}\")\n",
    "print(df[[\"title\", \"url\", \"paper_id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "640f40e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>url</th>\n",
       "      <th>embedding</th>\n",
       "      <th>sparse_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f5dfaf5b-b5cb-4180-9012-38a67d76065b</td>\n",
       "      <td>One Model, Many Languages: Meta-learning for M...</td>\n",
       "      <td>We introduce an approach to multilingual speec...</td>\n",
       "      <td>http://arxiv.org/abs/2008.00768v1</td>\n",
       "      <td>[-0.11276143789291382, -0.10186947137117386, -...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77695258-3d71-4d21-b2cc-87cd3d168bd1</td>\n",
       "      <td>Latent linguistic embedding for cross-lingual ...</td>\n",
       "      <td>As the recently proposed voice cloning system,...</td>\n",
       "      <td>http://arxiv.org/abs/2010.03717v1</td>\n",
       "      <td>[-0.1165810078382492, -0.10325288772583008, 0....</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d6dba606-822b-4e6d-80ae-cd75ce369d0a</td>\n",
       "      <td>Speech Synthesis as Augmentation for Low-Resou...</td>\n",
       "      <td>Speech synthesis might hold the key to low-res...</td>\n",
       "      <td>http://arxiv.org/abs/2012.13004v1</td>\n",
       "      <td>[-0.11325249820947647, -0.08711898326873779, 0...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2e901a47-3432-4647-a6c0-6a0bc66f3688</td>\n",
       "      <td>Listening while Speaking and Visualizing: Impr...</td>\n",
       "      <td>Previously, a machine speech chain, which is b...</td>\n",
       "      <td>http://arxiv.org/abs/1906.00579v3</td>\n",
       "      <td>[-0.07727877050638199, -0.15208560228347778, 0...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e0d1070-3987-4e40-a9ac-ab06f0159ae9</td>\n",
       "      <td>Data Augmentation for Training Dialog Models R...</td>\n",
       "      <td>Speech-based virtual assistants, such as Amazo...</td>\n",
       "      <td>http://arxiv.org/abs/2006.05635v1</td>\n",
       "      <td>[-0.07090704143047333, -0.08922877162694931, 0...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eae5ae37-25ce-4eaa-afb5-5c5208b3a687</td>\n",
       "      <td>Improving Cross-Lingual Transfer Learning for ...</td>\n",
       "      <td>Transfer learning from high-resource languages...</td>\n",
       "      <td>http://arxiv.org/abs/2006.05474v2</td>\n",
       "      <td>[-0.04299315810203552, -0.03154001757502556, -...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b7b953f2-c101-4548-8b30-eac78e90f89f</td>\n",
       "      <td>DNN-Based Semantic Model for Rescoring N-best ...</td>\n",
       "      <td>The word error rate (WER) of an automatic spee...</td>\n",
       "      <td>http://arxiv.org/abs/2011.00975v1</td>\n",
       "      <td>[-0.09180767834186554, -0.12420336157083511, 0...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b5753acc-09d9-475a-b18c-c4f9e9c97c1a</td>\n",
       "      <td>Mitigating the Impact of Speech Recognition Er...</td>\n",
       "      <td>Spoken question answering (SQA) is challenging...</td>\n",
       "      <td>http://arxiv.org/abs/1904.07904v1</td>\n",
       "      <td>[-0.07777009904384613, -0.10933384299278259, -...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14da6b25-9c84-4a2f-afd1-5200c60e1290</td>\n",
       "      <td>Exploration of End-to-End ASR for OpenSTT -- R...</td>\n",
       "      <td>This paper presents an exploration of end-to-e...</td>\n",
       "      <td>http://arxiv.org/abs/2006.08274v2</td>\n",
       "      <td>[-0.09484799206256866, -0.10854411870241165, -...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1992f64b-18a1-40dc-a899-119ca7f644d8</td>\n",
       "      <td>On the Comparison of Popular End-to-End Models...</td>\n",
       "      <td>Recently, there has been a strong push to tran...</td>\n",
       "      <td>http://arxiv.org/abs/2005.14327v2</td>\n",
       "      <td>[-0.12300242483615875, -0.16253547370433807, 0...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               paper_id  \\\n",
       "0  f5dfaf5b-b5cb-4180-9012-38a67d76065b   \n",
       "1  77695258-3d71-4d21-b2cc-87cd3d168bd1   \n",
       "2  d6dba606-822b-4e6d-80ae-cd75ce369d0a   \n",
       "3  2e901a47-3432-4647-a6c0-6a0bc66f3688   \n",
       "4  0e0d1070-3987-4e40-a9ac-ab06f0159ae9   \n",
       "5  eae5ae37-25ce-4eaa-afb5-5c5208b3a687   \n",
       "6  b7b953f2-c101-4548-8b30-eac78e90f89f   \n",
       "7  b5753acc-09d9-475a-b18c-c4f9e9c97c1a   \n",
       "8  14da6b25-9c84-4a2f-afd1-5200c60e1290   \n",
       "9  1992f64b-18a1-40dc-a899-119ca7f644d8   \n",
       "\n",
       "                                               title  \\\n",
       "0  One Model, Many Languages: Meta-learning for M...   \n",
       "1  Latent linguistic embedding for cross-lingual ...   \n",
       "2  Speech Synthesis as Augmentation for Low-Resou...   \n",
       "3  Listening while Speaking and Visualizing: Impr...   \n",
       "4  Data Augmentation for Training Dialog Models R...   \n",
       "5  Improving Cross-Lingual Transfer Learning for ...   \n",
       "6  DNN-Based Semantic Model for Rescoring N-best ...   \n",
       "7  Mitigating the Impact of Speech Recognition Er...   \n",
       "8  Exploration of End-to-End ASR for OpenSTT -- R...   \n",
       "9  On the Comparison of Popular End-to-End Models...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  We introduce an approach to multilingual speec...   \n",
       "1  As the recently proposed voice cloning system,...   \n",
       "2  Speech synthesis might hold the key to low-res...   \n",
       "3  Previously, a machine speech chain, which is b...   \n",
       "4  Speech-based virtual assistants, such as Amazo...   \n",
       "5  Transfer learning from high-resource languages...   \n",
       "6  The word error rate (WER) of an automatic spee...   \n",
       "7  Spoken question answering (SQA) is challenging...   \n",
       "8  This paper presents an exploration of end-to-e...   \n",
       "9  Recently, there has been a strong push to tran...   \n",
       "\n",
       "                                 url  \\\n",
       "0  http://arxiv.org/abs/2008.00768v1   \n",
       "1  http://arxiv.org/abs/2010.03717v1   \n",
       "2  http://arxiv.org/abs/2012.13004v1   \n",
       "3  http://arxiv.org/abs/1906.00579v3   \n",
       "4  http://arxiv.org/abs/2006.05635v1   \n",
       "5  http://arxiv.org/abs/2006.05474v2   \n",
       "6  http://arxiv.org/abs/2011.00975v1   \n",
       "7  http://arxiv.org/abs/1904.07904v1   \n",
       "8  http://arxiv.org/abs/2006.08274v2   \n",
       "9  http://arxiv.org/abs/2005.14327v2   \n",
       "\n",
       "                                           embedding sparse_values  \n",
       "0  [-0.11276143789291382, -0.10186947137117386, -...          None  \n",
       "1  [-0.1165810078382492, -0.10325288772583008, 0....          None  \n",
       "2  [-0.11325249820947647, -0.08711898326873779, 0...          None  \n",
       "3  [-0.07727877050638199, -0.15208560228347778, 0...          None  \n",
       "4  [-0.07090704143047333, -0.08922877162694931, 0...          None  \n",
       "5  [-0.04299315810203552, -0.03154001757502556, -...          None  \n",
       "6  [-0.09180767834186554, -0.12420336157083511, 0...          None  \n",
       "7  [-0.07777009904384613, -0.10933384299278259, -...          None  \n",
       "8  [-0.09484799206256866, -0.10854411870241165, -...          None  \n",
       "9  [-0.12300242483615875, -0.16253547370433807, 0...          None  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a17f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
